{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf2fbnJYNgjF"
      },
      "source": [
        "# 0. Loading Essential Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZiLNEm9NgjK"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import when, col\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpSiAgFWNgjL"
      },
      "source": [
        "# 1. Initial Data Loading and Understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sghf-9gcNgjM"
      },
      "outputs": [],
      "source": [
        "# Create Spark session with configurations:\n",
        "# - Allocate 4GB memory to driver for better performance with large datasets\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"FlightDelayPrediction\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "njRFpeYQNgjM"
      },
      "outputs": [],
      "source": [
        "# Interactive file path input for local development\n",
        "file_path1 = input(\"Please enter the path for the file (without quotes!): \")\n",
        "\n",
        "try:\n",
        "    # Load the datasets using the provided local paths\n",
        "    df = spark.read.csv(file_path1,header=True,inferSchema=True,nullValue=\"NA\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"File not found. Please check the path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ongXWoy7NgjN"
      },
      "source": [
        "## 1.1 Data Cleaning and Checking NULL values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6Hb_zUqINgjN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remove forbidden variables (information unknown at takeoff)\n",
        "forbidden_cols = [\n",
        "    \"ArrTime\",        \n",
        "    \"ActualElapsedTime\", \n",
        "    \"AirTime\",        \n",
        "    \"TaxiIn\",         \n",
        "    \"Diverted\",       \n",
        "    \"CarrierDelay\",   \n",
        "    \"WeatherDelay\",\n",
        "    \"NASDelay\",\n",
        "    \"SecurityDelay\",\n",
        "    \"LateAircraftDelay\"\n",
        "]\n",
        "\n",
        "df_cleaned = df.drop(*forbidden_cols)\n",
        "\n",
        "# Check if forbidden columns were dropped from the dataframe - if nothing is shown then OK\n",
        "existing_forbidden_cols = [col for col in forbidden_cols if col in df_cleaned.columns]\n",
        "existing_forbidden_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter out canceled flights and ensure valid elapsed time\n",
        "df_cleaned = df_cleaned.filter(col(\"Cancelled\") == 0) \\\n",
        "    .drop(\"CancellationCode\", \"Cancelled\") \\\n",
        "    .filter(col(\"CRSElapsedTime\") > 0) \\\n",
        "    .distinct()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check and handle repeated values between scheduled and actual times\n",
        "def handle_repeated_times(df):\n",
        "    # Calculate percentage of matches between DepTime and CRSDepTime\n",
        "    match_df = df.withColumn(\"is_match\", \n",
        "        when(col(\"DepTime\") == col(\"CRSDepTime\"), 1).otherwise(0))\n",
        "    \n",
        "    match_pct = match_df.agg(avg(\"is_match\") * 100).collect()[0][0]\n",
        "    \n",
        "    # If more than 70% match, drop DepTime as it's redundant\n",
        "    if match_pct > 70:\n",
        "        df = df.drop(\"DepTime\")\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[Year: int, Month: int, DayofMonth: int, DayOfWeek: int, DepTime: int, CRSDepTime: int, CRSArrTime: int, UniqueCarrier: string, FlightNum: int, TailNum: string, CRSElapsedTime: int, ArrDelay: int, DepDelay: int, Origin: string, Dest: string, Distance: int, TaxiOut: string]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "handle_repeated_times(df_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "a_91nCjXNgjN"
      },
      "outputs": [],
      "source": [
        "# Function to analyze missing values\n",
        "def analyze_null_values(df):\n",
        "    total_rows = df.count()\n",
        "    for column in df.columns:\n",
        "        null_count = df.filter(F.col(column).isNull()).count()\n",
        "        percentage = (null_count/total_rows) * 100\n",
        "        print(f\"{column}: {null_count} nulls ({percentage:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hwZaERtNgjO",
        "outputId": "0413c989-c9c3-489c-b325-11565e97343c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Year: 0 nulls (0.00%)\n",
            "Month: 0 nulls (0.00%)\n",
            "DayofMonth: 0 nulls (0.00%)\n",
            "DayOfWeek: 0 nulls (0.00%)\n",
            "DepTime: 0 nulls (0.00%)\n",
            "CRSDepTime: 0 nulls (0.00%)\n",
            "CRSArrTime: 0 nulls (0.00%)\n",
            "UniqueCarrier: 0 nulls (0.00%)\n",
            "FlightNum: 0 nulls (0.00%)\n",
            "TailNum: 5010652 nulls (100.00%)\n",
            "CRSElapsedTime: 0 nulls (0.00%)\n",
            "ArrDelay: 10333 nulls (0.21%)\n",
            "DepDelay: 0 nulls (0.00%)\n",
            "Origin: 0 nulls (0.00%)\n",
            "Dest: 0 nulls (0.00%)\n",
            "Distance: 6794 nulls (0.14%)\n",
            "TaxiOut: 5010652 nulls (100.00%)\n"
          ]
        }
      ],
      "source": [
        "# Check null values\n",
        "analyze_null_values(df_cleaned)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt0oqZRGNgjO"
      },
      "source": [
        "## 1.2 Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Dbcl1s2LNgjP"
      },
      "outputs": [],
      "source": [
        "# Create time-based features\n",
        "df_time = df_cleaned \\\n",
        "    .withColumn(\"DepTime_Hour\", F.floor(F.col(\"DepTime\")/100)) \\\n",
        "    .withColumn(\"DepTime_Minute\", F.col(\"DepTime\") % 100) \\\n",
        "    .withColumn(\"CRSDepTime_Hour\", F.floor(F.col(\"CRSDepTime\")/100)) \\\n",
        "    .withColumn(\"CRSDepTime_Minute\", F.col(\"CRSDepTime\") % 100) \\\n",
        "    .withColumn(\"CRSArrTime_Hour\", F.floor(F.col(\"CRSArrTime\")/100)) \\\n",
        "    .withColumn(\"CRSArrTime_Minute\", F.col(\"CRSArrTime\") % 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ri_gDdLrNgjP"
      },
      "outputs": [],
      "source": [
        "# Create categorical features\n",
        "df_categorical = df_time \\\n",
        "    .withColumn(\"IsWeekend\", F.when(F.col(\"DayOfWeek\").isin([6, 7]), 1).otherwise(0)) \\\n",
        "    .withColumn(\"TimeOfDay\",\n",
        "        F.when((F.col(\"DepTime_Hour\") >= 5) & (F.col(\"DepTime_Hour\") < 12), \"morning\")\n",
        "        .when((F.col(\"DepTime_Hour\") >= 12) & (F.col(\"DepTime_Hour\") < 17), \"afternoon\")\n",
        "        .when((F.col(\"DepTime_Hour\") >= 17) & (F.col(\"DepTime_Hour\") < 22), \"evening\")\n",
        "        .otherwise(\"night\")) \\\n",
        "    .withColumn(\"IsDelayed\", F.when(F.col(\"DepDelay\") > 0, 1).otherwise(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ePi5kDEINgjP"
      },
      "outputs": [],
      "source": [
        "# Create distance-based features, source: https://www.airliners.net/forum/viewtopic.php?t=461349\n",
        "df_distance = df_categorical \\\n",
        "    .withColumn(\"DistanceCategory\",\n",
        "        F.when(F.col(\"Distance\") <= 750, \"short\")\n",
        "        .when((F.col(\"Distance\") > 750) & (F.col(\"Distance\") <= 2500), \"medium\")\n",
        "        .otherwise(\"long\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RwrlRH1tNgjP"
      },
      "outputs": [],
      "source": [
        "# Create route frequency features - Between origin and destination what is the count of flight during year X.\n",
        "route_frequencies = df_distance.groupBy(\"Origin\", \"Dest\") \\\n",
        "    .count() \\\n",
        "    .withColumnRenamed(\"count\", \"RouteFrequency\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "j1YNqsANNgjP"
      },
      "outputs": [],
      "source": [
        "# Add route frequencies to main dataframe\n",
        "df_final = df_distance.join(route_frequencies, [\"Origin\", \"Dest\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfa7qIwLNgjP"
      },
      "source": [
        "## 1.3 Selected Features for Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iTSj7mTaNgjP"
      },
      "outputs": [],
      "source": [
        "features = [\n",
        "    # Time-related features\n",
        "    \"Month\",                 # Month of flight\n",
        "    \"DayOfWeek\",            # Day of week (1=Monday, 7=Sunday)\n",
        "    \"DepTime_Hour\",         # Hour of departure\n",
        "    \"DepTime_Minute\",       # Minute of departure\n",
        "    \"CRSDepTime_Hour\",      # Scheduled departure hour\n",
        "    \"CRSDepTime_Minute\",    # Scheduled departure minute\n",
        "\n",
        "    # Flight-specific features\n",
        "    \"UniqueCarrier\",        # Airline identifier\n",
        "    \"Distance\",             # Flight distance in miles\n",
        "    \"CRSElapsedTime\",      # Scheduled flight time\n",
        "\n",
        "    # Derived features\n",
        "    \"IsWeekend\",           # Weekend indicator\n",
        "    \"TimeOfDay\",           # Time period of day\n",
        "    \"DistanceCategory\",    # Flight distance category\n",
        "\n",
        "    # Route information\n",
        "    \"Origin\",              # Origin airport\n",
        "    \"Dest\",               # Destination airport\n",
        "    \"RouteFrequency\",     # Frequency of route\n",
        "\n",
        "    # Delay information\n",
        "    \"DepDelay\",           # Departure delay in minutes\n",
        "    \"IsDelayed\",          # Binary delay indicator\n",
        "\n",
        "    # Target variable\n",
        "    \"ArrDelay\"            # Arrival delay in minutes\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7tG1g_8NgjP",
        "outputId": "572251e6-c611-4bad-fc91-2a335c6c7160"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Month: integer (nullable = true)\n",
            " |-- DayOfWeek: integer (nullable = true)\n",
            " |-- DepTime_Hour: long (nullable = true)\n",
            " |-- DepTime_Minute: integer (nullable = true)\n",
            " |-- CRSDepTime_Hour: long (nullable = true)\n",
            " |-- CRSDepTime_Minute: integer (nullable = true)\n",
            " |-- UniqueCarrier: string (nullable = true)\n",
            " |-- Distance: integer (nullable = true)\n",
            " |-- CRSElapsedTime: integer (nullable = true)\n",
            " |-- IsWeekend: integer (nullable = false)\n",
            " |-- TimeOfDay: string (nullable = false)\n",
            " |-- DistanceCategory: string (nullable = false)\n",
            " |-- Origin: string (nullable = true)\n",
            " |-- Dest: string (nullable = true)\n",
            " |-- RouteFrequency: long (nullable = false)\n",
            " |-- DepDelay: integer (nullable = true)\n",
            " |-- IsDelayed: integer (nullable = false)\n",
            " |-- ArrDelay: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create final dataset\n",
        "model_data = df_final.select(features)\n",
        "\n",
        "# Show schema of final dataset\n",
        "model_data.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1UYU8IeNgjQ",
        "outputId": "e821297e-3130-42c4-8dbd-4f113c1e33de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary statistics for numerical columns:\n",
            "+-------+-----------------+------------------+------------------+------------------+\n",
            "|summary|         Distance|    CRSElapsedTime|          DepDelay|          ArrDelay|\n",
            "+-------+-----------------+------------------+------------------+------------------+\n",
            "|  count|          5003858|           5010652|           5010652|           5000319|\n",
            "|   mean|670.8173859050356|115.09698917426314| 6.122558301793858| 5.344012252018321|\n",
            "| stddev|522.8019881679623|  64.4886595947797|21.395056670055883|22.994638451256556|\n",
            "|    min|               11|                 2|              -675|              -829|\n",
            "|    25%|              297|                68|                -1|                -6|\n",
            "|    50%|              510|                97|                 0|                 0|\n",
            "|    75%|              913|               147|                 5|                 9|\n",
            "|    max|             4502|              1613|              1439|              1291|\n",
            "+-------+-----------------+------------------+------------------+------------------+\n",
            "\n",
            "Correlation between Distance and ArrDelay: 0.022\n",
            "Correlation between CRSElapsedTime and ArrDelay: 0.023\n",
            "Correlation between DepDelay and ArrDelay: 0.764\n",
            "Correlation between DepTime_Hour and ArrDelay: 0.131\n",
            "Correlation between RouteFrequency and ArrDelay: 0.021\n",
            "\n",
            "Distribution of TimeOfDay:\n",
            "+---------+------------------+-------+\n",
            "|TimeOfDay|         avg_delay|  count|\n",
            "+---------+------------------+-------+\n",
            "|afternoon| 5.130050189436488|1583308|\n",
            "|    night|17.634905184479955| 141330|\n",
            "|  morning|2.2461277147688055|1946734|\n",
            "|  evening|  8.80593544100548|1339280|\n",
            "+---------+------------------+-------+\n",
            "\n",
            "\n",
            "Distribution of DistanceCategory:\n",
            "+----------------+-----------------+-------+\n",
            "|DistanceCategory|        avg_delay|  count|\n",
            "+----------------+-----------------+-------+\n",
            "|            long|4.135822843107154|  52369|\n",
            "|          medium|6.203304141384166|1588712|\n",
            "|           short|4.958224214036405|3369571|\n",
            "+----------------+-----------------+-------+\n",
            "\n",
            "\n",
            "Distribution of UniqueCarrier:\n",
            "+-------------+------------------+------+\n",
            "|UniqueCarrier|         avg_delay| count|\n",
            "+-------------+------------------+------+\n",
            "|           UA| 6.793608530283474|642173|\n",
            "|           AA| 6.079753335679816|777425|\n",
            "|           NW|  1.94920382199773|466950|\n",
            "|           HP| 4.796905123144842|175721|\n",
            "|           TW|3.6247267770916616|241250|\n",
            "|           DL| 8.635281891634126|890515|\n",
            "|           US| 4.194492783419364|818569|\n",
            "|           AS|4.9617426303738466| 97792|\n",
            "|           CO| 5.615907670998019|414190|\n",
            "|           WN|2.3155004625804363|486067|\n",
            "+-------------+------------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Additional exploratory analysis\n",
        "# Display summary statistics for numerical columns\n",
        "print(\"Summary statistics for numerical columns:\")\n",
        "df_final.select(\"Distance\", \"CRSElapsedTime\", \"DepDelay\", \"ArrDelay\").summary().show()\n",
        "\n",
        "# Calculate correlations with target variable\n",
        "numeric_cols = [\"Distance\", \"CRSElapsedTime\", \"DepDelay\", \"DepTime_Hour\", \"RouteFrequency\"]\n",
        "for col in numeric_cols:\n",
        "    correlation = df_final.stat.corr(col, \"ArrDelay\")\n",
        "    print(f\"Correlation between {col} and ArrDelay: {correlation:.3f}\")\n",
        "\n",
        "# Analyze categorical variables distribution\n",
        "categorical_cols = [\"TimeOfDay\", \"DistanceCategory\", \"UniqueCarrier\"]\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\nDistribution of {col}:\")\n",
        "    df_final.groupBy(col).agg(\n",
        "        F.avg(\"ArrDelay\").alias(\"avg_delay\"),\n",
        "        F.count(\"*\").alias(\"count\")\n",
        "    ).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZwGF2oVNgjQ"
      },
      "source": [
        "## 1.4 Saving Processed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "N5U4EevRNgjQ"
      },
      "outputs": [],
      "source": [
        "# Use current working directory as a substitute for script directory\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "# Define the path to save the file\n",
        "save_path = os.path.join(current_dir, \"processed_data\")\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Save the file\n",
        "model_data.write.parquet(save_path, mode=\"overwrite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-qtyc_6NgjQ"
      },
      "source": [
        "# 2. Model Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4FERPjCNgjQ"
      },
      "source": [
        "## 2.1 Prepare features for modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6EobH1SrNgjR"
      },
      "outputs": [],
      "source": [
        "# First drop rows with nulls in important columns\n",
        "model_data_cleaned = model_data.dropna(subset=[\"ArrDelay\", \"DepDelay\", \"DepTime_Hour\", \"DepTime_Minute\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lzfxVal8NgjR"
      },
      "outputs": [],
      "source": [
        "# Index categorical columns\n",
        "categorical_cols = [\"TimeOfDay\", \"DistanceCategory\", \"UniqueCarrier\", \"Origin\", \"Dest\"]\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_idx\", handleInvalid=\"keep\")\n",
        "           for col in categorical_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qh0MWbozNgjR"
      },
      "outputs": [],
      "source": [
        "# Create encoders for indexed variables\n",
        "encoders = [OneHotEncoder(inputCol=f\"{col}_idx\", outputCol=f\"{col}_encoded\")\n",
        "           for col in categorical_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bw26q8nZNgjR"
      },
      "outputs": [],
      "source": [
        "# Define numeric features\n",
        "numeric_cols = [\"Month\", \"DayOfWeek\", \"Distance\", \"CRSElapsedTime\",\n",
        "               \"DepTime_Hour\", \"DepTime_Minute\", \"RouteFrequency\", \"DepDelay\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "AJcg4fMpNgjR"
      },
      "outputs": [],
      "source": [
        "# Combine all features\n",
        "feature_cols = numeric_cols + [f\"{col}_encoded\" for col in categorical_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Ncsw_DOUNgjR"
      },
      "outputs": [],
      "source": [
        "# Create feature vector with null handling\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=numeric_cols + [f\"{col}_encoded\" for col in categorical_cols],\n",
        "    outputCol=\"features\",\n",
        "    handleInvalid=\"skip\"  # Handle null values\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gBt1vO2NgjR"
      },
      "source": [
        "## 2.2 Create and configure Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "X1z6xenNNgjR"
      },
      "outputs": [],
      "source": [
        "# Create different models\n",
        "\n",
        "# Linear Regression\n",
        "lr = LinearRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"ArrDelay\",\n",
        "    maxIter=10\n",
        ")\n",
        "\n",
        "# Decision Tree\n",
        "dt = DecisionTreeRegressor(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"ArrDelay\",\n",
        "    maxDepth=5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRb8igWhNgjS"
      },
      "source": [
        "## 2.3 Create Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "oWyS3wutVtbq"
      },
      "outputs": [],
      "source": [
        "# Create pipelines for each model\n",
        "pipeline_lr = Pipeline(stages=indexers + encoders + [assembler, lr])\n",
        "pipeline_dt = Pipeline(stages=indexers + encoders + [assembler, dt])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtXrhgVNNgjS"
      },
      "source": [
        "## 2.4 Split data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "rqENEkBMNgjS"
      },
      "outputs": [],
      "source": [
        "# Split data into training and test sets:\n",
        "# - 80% for training (train_data)\n",
        "# - 20% for testing (test_data)\n",
        "# - seed=42 ensures reproducibility of the random split\n",
        "train_data, test_data = model_data_cleaned.randomSplit([0.8, 0.2], seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4Ueyc2VNgjT"
      },
      "source": [
        "## 2.5 Train and evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Jh1dUlP2WXD-"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate each model\n",
        "models = {\n",
        "    \"Linear Regression\": pipeline_lr,\n",
        "    \"Decision Tree\": pipeline_dt\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate_models(models, train_data, test_data):\n",
        "\n",
        "    \"\"\"\n",
        "    Trains multiple models and evaluates their performance using regression metrics\n",
        "    \n",
        "    Args:\n",
        "        models: Dictionary of model pipelines to train\n",
        "        train_data: Training dataset\n",
        "        test_data: Test dataset for evaluation\n",
        "    Returns:\n",
        "        model_metrics: Dictionary containing metrics for each model\n",
        "        best_model_name: Name of the best performing model\n",
        "    \"\"\"\n",
        "\n",
        "    # Print header for training process\n",
        "    print(\"\\nTraining and Evaluating Models...\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Initialize evaluator for regression metrics\n",
        "    evaluator = RegressionEvaluator(\n",
        "        labelCol=\"ArrDelay\",        # Target variable\n",
        "        predictionCol=\"prediction\"  # Predicted values column\n",
        "    )\n",
        "    \n",
        "    # Initialize storage for metrics and tracking best model\n",
        "    model_metrics = {}\n",
        "    best_model_name = None\n",
        "    best_rmse = float('inf')\n",
        "    \n",
        "    # Train and evaluate each model in the dictionary\n",
        "    for name, pipeline in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "\n",
        "        # Fit model pipeline on training data\n",
        "        model = pipeline.fit(train_data)\n",
        "\n",
        "        # Generate predictions on test data\n",
        "        predictions = model.transform(test_data)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions) # Root Mean Square Error\n",
        "        mae = evaluator.setMetricName(\"mae\").evaluate(predictions) # Mean Absolute Error\n",
        "        r2 = evaluator.setMetricName(\"r2\").evaluate(predictions) # R-squared score\n",
        "        \n",
        "        # Store all metrics and model object\n",
        "        model_metrics[name] = {\n",
        "            \"RMSE\": rmse,\n",
        "            \"MAE\": mae,\n",
        "            \"R2\": r2,\n",
        "            \"model\": model\n",
        "        }\n",
        "        \n",
        "        # Update best model if current RMSE is lower\n",
        "        if rmse < best_rmse:\n",
        "            best_rmse = rmse\n",
        "            best_model_name = name\n",
        "        \n",
        "        # Print metrics for current model\n",
        "        print(f\"{name} Metrics:\")\n",
        "        print(f\"RMSE: {rmse:.2f}\")\n",
        "        print(f\"MAE: {mae:.2f}\")\n",
        "        print(f\"R2: {r2:.3f}\")\n",
        "    \n",
        "    return model_metrics, best_model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_model(model, data, evaluator):\n",
        "    \n",
        "    \"\"\"\n",
        "    Performs comprehensive model validation including both standard regression metrics\n",
        "    and business-specific accuracy metrics\n",
        "    \n",
        "    Args:\n",
        "        model: Trained ML model to evaluate\n",
        "        data: Dataset to validate against\n",
        "        evaluator: RegressionEvaluator instance for calculating metrics\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing all validation metrics\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate predictions on validation data\n",
        "    predictions = model.transform(data)\n",
        "    \n",
        "    # Calculate standard metrics\n",
        "    rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n",
        "    mae = evaluator.setMetricName(\"mae\").evaluate(predictions)\n",
        "    r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n",
        "    \n",
        "    # Calculate business metrics\n",
        "    total_flights = predictions.count()\n",
        "    \n",
        "    # Calculate percentage of predictions within 15 minutes of actual delay\n",
        "    accurate_predictions = predictions.filter(\n",
        "        F.abs(F.col(\"prediction\") - F.col(\"ArrDelay\")) <= 15\n",
        "    ).count()\n",
        "    prediction_accuracy = accurate_predictions / total_flights\n",
        "    \n",
        "    # Calculate accuracy for severe delays (>60 minutes)\n",
        "    severe_delays = predictions.filter(F.col(\"ArrDelay\") > 60)\n",
        "    if severe_delays.count() > 0:\n",
        "        severe_correct = severe_delays.filter(\n",
        "            F.col(\"prediction\") > 60\n",
        "        ).count()\n",
        "        severe_delay_accuracy = severe_correct / severe_delays.count()\n",
        "    else:\n",
        "        severe_delay_accuracy = 0\n",
        "    \n",
        "    return {\n",
        "        \"RMSE\": rmse,\n",
        "        \"MAE\": mae,\n",
        "        \"R2\": r2,\n",
        "        \"15min_Accuracy\": prediction_accuracy,\n",
        "        \"Severe_Delay_Accuracy\": severe_delay_accuracy\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training and Evaluating Models...\n",
            "--------------------------------------------------\n",
            "\n",
            "Training Linear Regression...\n",
            "Linear Regression Metrics:\n",
            "RMSE: 14.66\n",
            "MAE: 8.04\n",
            "R2: 0.592\n",
            "\n",
            "Training Decision Tree...\n",
            "Decision Tree Metrics:\n",
            "RMSE: 14.29\n",
            "MAE: 8.53\n",
            "R2: 0.612\n",
            "\n",
            "==================================================\n",
            "Best Performing Model: Decision Tree\n",
            "Best RMSE: 14.29\n",
            "Best MAE: 8.53\n",
            "Best R2: 0.612\n",
            "==================================================\n",
            "\n",
            "Performing Detailed Model Validation...\n",
            "--------------------------------------------------\n",
            "\n",
            "Detailed Validation Results:\n",
            "--------------------------------------------------\n",
            "Metric\t\t\tTraining\tTest\n",
            "--------------------------------------------------\n",
            "RMSE           \t14.351\t14.294\n",
            "MAE            \t8.548\t8.526\n",
            "R2             \t0.611\t0.612\n",
            "15min_Accuracy \t0.861\t0.861\n",
            "Severe_Delay_Accuracy\t0.801\t0.797\n",
            "\n",
            "Model and validation metrics saved to 'best_model' directory\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate all models using the helper function\n",
        "# Returns metrics dictionary and name of best performing model\n",
        "model_metrics, best_model_name = train_and_evaluate_models(models, train_data, test_data)\n",
        "best_model = model_metrics[best_model_name][\"model\"]\n",
        "\n",
        "# Display results header for best model\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(f\"Best Performing Model: {best_model_name}\")\n",
        "print(f\"Best RMSE: {model_metrics[best_model_name]['RMSE']:.2f}\")\n",
        "print(f\"Best MAE: {model_metrics[best_model_name]['MAE']:.2f}\")\n",
        "print(f\"Best R2: {model_metrics[best_model_name]['R2']:.3f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Perform detailed validation on both training and test sets\n",
        "print(\"\\nPerforming Detailed Model Validation...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Initialize regression evaluator for metrics calculation\n",
        "evaluator = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\")\n",
        "\n",
        "# Get detailed metrics for both training and test data\n",
        "train_metrics = validate_model(best_model, train_data, evaluator)\n",
        "test_metrics = validate_model(best_model, test_data, evaluator)\n",
        "\n",
        "# Display validation results\n",
        "print(\"\\nDetailed Validation Results:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Metric\\t\\t\\tTraining\\tTest\")\n",
        "print(\"-\" * 50)\n",
        "for metric in train_metrics.keys():\n",
        "    print(f\"{metric:<15}\\t{train_metrics[metric]:.3f}\\t{test_metrics[metric]:.3f}\")\n",
        "\n",
        "# Save model and metrics to disk\n",
        "# Create directory if it doesn't exist\n",
        "if not os.path.exists(\"best_model\"):\n",
        "    os.makedirs(\"best_model\")\n",
        "\n",
        "# Save the model\n",
        "best_model.write().overwrite().save(\"best_model\")\n",
        "\n",
        "# Prepare validation results dictionary\n",
        "validation_results = {\n",
        "    \"model_type\": best_model_name,\n",
        "    \"training_metrics\": train_metrics,\n",
        "    \"test_metrics\": test_metrics\n",
        "}\n",
        "\n",
        "# Save metrics as JSON file\n",
        "with open(\"best_model/metrics.json\", \"w\") as f:\n",
        "    json.dump(validation_results, f, indent=4)\n",
        "\n",
        "print(\"\\nModel and validation metrics saved to 'best_model' directory\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
